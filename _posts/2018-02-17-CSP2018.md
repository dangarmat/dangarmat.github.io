---
layout: post
title: ASA Conference on Statistical Practice 2018
category: [R, ASA, CSP2018]
tags: [R, ASA, CSP2018]
---

![CSP Conf Logo](/images/csp2018.png "Conference Logo")

Highlights from [Conference on Statistical Practice](https://ww2.amstat.org/meetings/csp/2018/index.cfm) sessions. To keep it simple just a few notes and my favorite slide per session I attended. I work in Population Health Analytics at Legacy Hospital System in Portland, Oregon, so some of these presenters' work may be interpritted in terms of that lense.

Posts by time period:
**Friday 2/16/2018**
* 8:00 AM Keynote Address & 9:15 AM Working with Messy Data 
* 11:00 AM Streamlining Your Work Using (Shiny) Apps
* 2:00 PM Data Mining Algorithms / Presenting and Storytelling 
* 3:45 PM Working with Health Care Data
**Saturday 2/17/2018**
* 9:15 AM Poster Session 3 / Survival Analysis v. 'Survival' Analysis
* 11:00 AM Causal Inference 
* 2:00 PM Deploying Quantitative Models as 'Visuals' in Popular Data Visualization Platforms 
* Additional Sessions I Wish I'd Attended

## Friday 2/16/2018

## 8:00 AM Keynote Address

### Reflections on Career Opportunities and Leadership in Statistics, *Lisa LaVange, The University of North Carolina*

Leadership - to some extent just do it - statistical training alone provides perspective and a right to contribute. There are "statistical leadership" opportunities and I felt inspired to improve art of of contributing.

Precision medicine is determining the right intervention at right time at the right dose. It struck me care management is precision medecine, or could become it.

## 9:15 AM Working with Messy Data 

### Practical Time-Series Clustering for Messy Data in R *[Jonathan Robert Page](http://www2.hawaii.edu/~jrpage/), University of Hawaii Economic Research Organization*

Problem: a Kenyan GoFundMe type site wants to know if there are clusters of projects in terms of funding patterns to get a sense which need promotion and which do not.

He took time series data on donations to projects and looked for clusterings using two methods: Dynamic Time Warping (DTW) using [dtwclust R package](https://cran.r-project.org/web/packages/dtwclust/index.html) and k-Shapes. He uses cumulative sums, not daily donation amounts as the y-value.

After cleaning (for which he presents [a wealth of R code](https://github.com/jonpage/csp2018)) it feels easy (maybe he makes it look easy). DTW seems to cluster on when changes occur, so assumes similar changes and measures time distance, x-distance between the key changes, whereas k-shape seems to measure vertical distance at the same time - but not sure these are correct interpritations. 

He likes to present clusters of Time Series in x by x style - so 4, 9, or 16 clusters. He does this to show stakeholders different resolutions / zoom levels. 

![Clusters of donations with k = 9 k-Shapes](/images/smokelines.png "Clusters of donations with k = 9 k-Shapes")

Pretty "smoke graph" I'd call it, with alpha = 0.1. Blue line is medoid of normalized cluster. Can see in the second one a cluster where no one ever donates anything in the first 30 days. Or Cluster 5, does great at first then flattens out. Clusters 8 and 9 look the most promising - since they finish their first 30 days still with an upward slope, that may indicate continuing interest.

### 	Doing Data Linkage: A Behind-the-Scenes Look *[Clinton J. Thompson](https://www.researchgate.net/profile/Clinton_Thompson), National Center for Health Statistics, CDC*

Problem: full story of health and death is in two datasets, which require being linked, but these files have millions of rows and no common ID, and thus trillions of possible combinations, so linkage isn't straightforward. 

Data linkage processes can enhance review and quality control process. He uses dashboards to QC. I see this being a useful way to monitor data for possible errors - a dashboard designed for the analytics team that attempts to answer the question "can I trust the new data?" Could program in checks for the kinds of messes we've seen before, and maybe even do processing in PowerBI with check boxes to make suggested corrections. Maybe a top 5 correction suggestions, and check yes to fix it in the output file.

## 11:00 AM Streamlining Your Work Using Apps

### Mechanizing Clinical Review Processes with R Shiny for Efficiency and Standardization *Jimmy Wong, Food and Drug Administration*

### Building Shiny Apps: With Great Power Comes Great Responsibility *Jessica Minnier, Oregon Health & Science University*

## 2:00 PM Data Mining Algorithms / Presenting and Storytelling 

### Stochastic Gradient Boosting on Distributed Data *Roxy Cramer, Rogue Wave Software *

### Telling the Story of Your Stats *Jennifer H. Van Mullekom, Virginia Tech*

## 3:45 PM Working with Health Care Data

### Application of Support Vector Machine Modeling and Graph Theory Metrics for Disease Classification *Jessica Michelle Rudd, Kennesaw State University*

### Assessing Correspondence Between Two Data Sources Across Categorical Covariates with Missing Data: Application to Electronic Health Records *Emile Latour, Oregon Health & Science University*


## Saturday 2/17/2018

## 9:15 AM Poster Session 3 / Survival Analysis v. 'Survival' Analysis

### Exploring Data Quality and Time Series Event Detection in 2016 US Presidential Election Polls, *[Kaelyn	M.	Rosenburg](https://ww2.amstat.org/meetings/csp/2018/onlineprogram/AbstractDetails.cfm?AbstractID=303685), Reed College*
Problem: Polling implied Clinton would beat Trump. Could survey mode effect explain this error? 

![Polling Clinton v. Trump by phone and by web](/images/clintontrumppolls.png "Polling over time")

This cool plot shows how web vs. phone polling coverged for Clinton over time but Trump's web polling consistently remained higher than phone polling. She suggests this visualizes a possible social desirability bias that could have led to variability in polling estimates.

### Statistical Methods for National Security Risk Quantification and Optimal Resource Allocation, *[Robert Brigantic](https://www.pnnl.gov/science/staff/staff_info.asp?staff_num=8633), Pacific Northwest National Laboratory*

Problem: given multiple risk points and danger situations, how do we best allocate resources to reduce risk? Can we do this intelligently based on data and logic, to supplement expert intuition? 

Pacific Northwest National Lab attempted to solve this problem by quantifying risk. They break it down into quantifiable sub-problems. The final equation, I thought was a nice way to represent it: R = f(C,V,T). This means risk is a function of Consequence, Vulnerability, and Threat. Consequence means how severe is an event - especially in terms of death or injury. Vulnerability means how easy to exploit is a given location? And Threat means likelihood of natural or man-made occurance with potential to harm life, information, or property.

Breaking risk down into these three components allows further breaking it down into measurable sub-components, until it's at a level that analysis can be done. Here they have taken the first component, C, Consequence, broken it down into seven categories, broken a site down into five areas, looked at three possible scenarios and calculated the consequence level of that type of event occuring in that area on that category of consequence on a scale of 1 to 5. The numerical estimate comes with help from data and stakeholders.

![Breaking down risk](/images/pnnl01.png "Risk heat map")

I think this systematic approach could be used to model risk in other questions, such as risk of a hospital admission, or risk of diabetes worsening to other conditions such as end stage renal disease. Instead of five areas, each member of a population would be an "area". Each "security threat" would be a condition that a member could obtain, such as renal disease or cariovascular disease. Consequence could be separated into [the quadruple aim](http://www.annfammed.org/content/12/6/573.full), vulnerability could come from chronic conditions. Threat would be the liklihood of each one, perhaps based on data.

## 11:00 AM Causal Inference 

### Causal Inference with Multilevel Data Structures, *[Luke Keele](http://lukekeele.com/), Georgetown University*

Problem: observational studies often are the only realistic option, with bias than prevents normal statistical methods from being reliable. When observational studies have clustering occuring among who receives or doesn't recieve a treatment, that introduces a multilayered issue that makes it yet more complex.

He thinks multilevel matching is more ideal than multilevel regression for clustered observational studies. He has an R package [multiMatch/matchMulti](https://cran.r-project.org/web/packages/matchMulti/index.html) that does this matching work and has several options to handle more complex cases. 

The second study shown was cool. It looked at an actual RCT, but with bias in schools that choose to particpate. He removed the randomly assigned control group, and tried to see if the randomly assigned treatment group could show the same effect in comparison to the non-participating group after adjusting for selection bias. Since the effect size was known, it is a cool example case to see how well the matching algorithm works to get at the treatment effect in an observational study setting. 

Since students could be matched or schools could be matched, it is a multi-level situation requiring more complex matching either by student or school or both. Don't remember the results though.


### A Decision Tool for Causal Inference and Observational Data Analysis Methods in Comparative Effectiveness Research, *[Douglas Landsittel](http://www.dbmi.pitt.edu/node/52371), University of Pittsburgh*

Propensity scores are a prediction problem, but predict on treatment - as opposed to on the response. So they predict if someone received the treatment. In a randomized control trial, an individual's probablity of treatment or control is the same. He suggested propensity scores may be a perfect application of machine learning, since a black box is fine. I've heard this before, and think random forest does make sense, to handle automatic non-linearity managing, and getting to a number quickly. 

He emphasized people use propensity score **methods** not just propensity scores. 
Once having a propensity score, there are four approaches to using them. Of 4 approaches, adjust is the worst. 
One method is inverse of propensity probability as a weight. I thought this looked interesting. There is a lot of work on pros and cons of these methods. He thinks it depends on area of common support and distribution of propensity scores.

## 2:00 PM Deploying Quantitative Models as 'Visuals' in Popular Data Visualization Platforms 

### Deploying Quantitative Models as 'Visuals' in Popular Data Visualization Platforms, *[Daniel Fylstra](https://www.solver.com/node/7156), Frontline Systems Inc.*

I use PowerBI at work, and think putting a model in PowerBI may make it less mysterious. I liked his comment of meeting users on their own territory, so to speak. Ideally PowerBI, Tableau, Qlikview, etc. enable "what if" on the spot and rerunning a model in front of stakeholders, leading to a stronger case to implement it.

PowerBI uses TypeScript, a higher level than JavaScript.

His advice about "big data" is it could be a bubble - that if you want to learn Spark or Python or R to use it right now, go for it, but that it seems in two years they may be replaced by something else (I'm thinking Julia and SQL wrappers are contenters). So what matters in his view is the fundamentals. I interpritted this as the fundamentals of problem solving with data. 


