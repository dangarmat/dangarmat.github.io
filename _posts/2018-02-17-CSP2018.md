---
layout: post
title: ASA Conference on Statistical Practice 2018
category: [R, fitbit]
tags: [R, fitbit]
---

![CSP Conf Logo](/images/csp2018.png "Conference Logo")

Highlights from [Conference on Statistical Practice](https://ww2.amstat.org/meetings/csp/2018/index.cfm) sessions I attended. To keep it simple just a few notes and my favorite slide per session.

## Friday 2/16/2018



## Saturday 2/17/2018


### Statistical Methods for National Security Risk Quantification and Optimal Resource Allocation, *Robert Brigantic, Pacific Northwest National Laboratory*

Problem: given multiple risk points and danger situations, how do we best allocate resources to reduce risk? Can we do this intelligently based on data and logic, to supplement expert intuition? 

Pacific Northwest National Lab attempted to solve this problem by quantifying risk. They break it down into quantifiable sub-problems. The final equation, I thought was a nice way to represent it: R = f(C,V,T). This means risk is a function of Consequence, Vulnerability, and Threat. Consequence means how severe is an event - especially in terms of death or injury. Vulnerability means how easy to exploit is a given location? And Threat means likelihood of natural or man-made occurance with potential to harm life, information, or property.

Breaking risk down into these three components allows further breaking it down into measurable sub-components, until it's at a level that analysis can be done. Here they have taken the first component, C, Consequence, broken it down into seven categories, broken a site down into five areas, looked at three possible scenarios and calculated the consequence level of that type of event occuring in that area on that category of consequence on a scale of 1 to 5. The numerical estimate comes with help from data and stakeholders.

![Breaking down risk](/images/pnnl01.png "Risk heat map")

I think this systematic approach could be used to model risk in other questions, such as risk of a hospital admission, or risk of diabetes worsening to other conditions such as end stage renal disease. Instead of five areas, each member of a population would be an "area". Each "security threat" would be a condition that a member could obtain, such as renal disease or cariovascular disease. Consequence could be separated into [the quadruple aim](http://www.annfammed.org/content/12/6/573.full), vulnerability could come from chronic conditions. Threat would be the liklihood of each one, perhaps based on data.


### Causal Inference with Multilevel Data Structures, *Luke Keele, Georgetown*

Problem: observational studies often are the only realistic option, with bias than prevents normal statistical methods from being reliable. When observational studies have clustering occuring among who receives or doesn't recieve a treatment, that introduces a multilayered issue that makes it yet more complex.

He thinks multilevel matching is more ideal than multilevel regression for clustered observational studies. He has an R package [multiMatch/matchMulti](https://cran.r-project.org/web/packages/matchMulti/index.html) that does this matching work and has several options to handle more complex cases. 

The second study shown was cool. It looked at an actual RCT, but with bias in schools that choose to particpate. He removed the randomly assigned control group, and tried to see if the randomly assigned treatment group could show the same effect in comparison to the non-participating group after adjusting for selection bias. Since the effect size was known, it is a cool example case to see how well the matching algorithm works to get at the treatment effect in an observational study setting. 

Since students could be matched or schools could be matched, it is a multi-level situation requiring more complex matching either by student or school or both. Don't remember the results though.



```r
require(tidyverse)
require(purrr)
```
