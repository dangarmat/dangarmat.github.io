---
layout: post
title: ASA Conference on Statistical Practice 2018
category: [R, fitbit]
tags: [R, fitbit]
---

![CSP Conf Logo](/images/csp2018.png "Conference Logo")

Highlights from [Conference on Statistical Practice](https://ww2.amstat.org/meetings/csp/2018/index.cfm) sessions I attended. To keep it simple just a few notes and my favorite slide per session.

## Friday 2/16/2018



## Saturday 2/17/2018


### Exploring Data Quality and Time Series Event Detection in 2016 US Presidential Election Polls, *[Kaelyn	M.	Rosenburg, Reed College](https://ww2.amstat.org/meetings/csp/2018/onlineprogram/AbstractDetails.cfm?AbstractID=303685)*
Problem: Polling implied Clinton would beat Trump. Could survey mode effect explain this error? 

![Polling Clinton v. Trump by phone and by web](/images/clintontrumppolls.png "Polling over time")

This cool plot shows how web vs. phone polling coverged for Clinton over time but Trump's web polling consistently remained higher than phone polling. She suggests this visualizes a possible social desirability bias that could have led to variability in polling estimates.

### Statistical Methods for National Security Risk Quantification and Optimal Resource Allocation, *[Robert Brigantic, Pacific Northwest National Laboratory](https://www.pnnl.gov/science/staff/staff_info.asp?staff_num=8633)*

Problem: given multiple risk points and danger situations, how do we best allocate resources to reduce risk? Can we do this intelligently based on data and logic, to supplement expert intuition? 

Pacific Northwest National Lab attempted to solve this problem by quantifying risk. They break it down into quantifiable sub-problems. The final equation, I thought was a nice way to represent it: R = f(C,V,T). This means risk is a function of Consequence, Vulnerability, and Threat. Consequence means how severe is an event - especially in terms of death or injury. Vulnerability means how easy to exploit is a given location? And Threat means likelihood of natural or man-made occurance with potential to harm life, information, or property.

Breaking risk down into these three components allows further breaking it down into measurable sub-components, until it's at a level that analysis can be done. Here they have taken the first component, C, Consequence, broken it down into seven categories, broken a site down into five areas, looked at three possible scenarios and calculated the consequence level of that type of event occuring in that area on that category of consequence on a scale of 1 to 5. The numerical estimate comes with help from data and stakeholders.

![Breaking down risk](/images/pnnl01.png "Risk heat map")

I think this systematic approach could be used to model risk in other questions, such as risk of a hospital admission, or risk of diabetes worsening to other conditions such as end stage renal disease. Instead of five areas, each member of a population would be an "area". Each "security threat" would be a condition that a member could obtain, such as renal disease or cariovascular disease. Consequence could be separated into [the quadruple aim](http://www.annfammed.org/content/12/6/573.full), vulnerability could come from chronic conditions. Threat would be the liklihood of each one, perhaps based on data.


### Causal Inference with Multilevel Data Structures, *[Luke Keele, Georgetown University](http://lukekeele.com/)*

Problem: observational studies often are the only realistic option, with bias than prevents normal statistical methods from being reliable. When observational studies have clustering occuring among who receives or doesn't recieve a treatment, that introduces a multilayered issue that makes it yet more complex.

He thinks multilevel matching is more ideal than multilevel regression for clustered observational studies. He has an R package [multiMatch/matchMulti](https://cran.r-project.org/web/packages/matchMulti/index.html) that does this matching work and has several options to handle more complex cases. 

The second study shown was cool. It looked at an actual RCT, but with bias in schools that choose to particpate. He removed the randomly assigned control group, and tried to see if the randomly assigned treatment group could show the same effect in comparison to the non-participating group after adjusting for selection bias. Since the effect size was known, it is a cool example case to see how well the matching algorithm works to get at the treatment effect in an observational study setting. 

Since students could be matched or schools could be matched, it is a multi-level situation requiring more complex matching either by student or school or both. Don't remember the results though.


### A Decision Tool for Causal Inference and Observational Data Analysis Methods in Comparative Effectiveness Research, *[Douglas Landsittel, University of Pittsburgh](http://www.dbmi.pitt.edu/node/52371)*

Propensity scores are a prediction problem, but predict on treatment - as opposed to on the response. So they predict if someone received the treatment. In a randomized control trial, an individual's probablity of treatment or control is the same. He suggested propensity scores may be a perfect application of machine learning, since a black box is fine. I've heard this before, and think random forest does make sense, to handle automatic non-linearity managing, and getting to a number quickly. 

He emphasized people use propensity score **methods** not just propensity scores. 
Once having a propensity score, there are four approaches to using them. Of 4 approaches, adjust is the worst. 
One method is inverse of propensity probability as a weight. I thought this looked interesting. There is a lot of work on pros and cons of these methods. He thinks it depends on area of common support and distribution of propensity scores.


### Deploying Quantitative Models as 'Visuals' in Popular Data Visualization Platforms, *[Daniel Fylstra, Frontline Systems Inc.](https://www.solver.com/node/7156)*

I use PowerBI at work, and think putting a model in PowerBI may make it less mysterious. I liked his comment of meeting users on their own territory, so to speak. Ideally PowerBI, Tableau, Qlikview, etc. enable "what if" on the spot and rerunning a model in front of stakeholders, leading to a stronger case to implement it.

PowerBI uses TypeScript, a higher level than JavaScript.

His advice about "big data" is it could be a bubble - that if you want to learn Spark or Python or R to use it right now, go for it, but that it seems in two years they may be replaced by something else (I'm thinking Julia and SQL wrappers are contenters). So what matters in his view is the fundamentals. I interpritted this as the fundamentals of problem solving with data. 


