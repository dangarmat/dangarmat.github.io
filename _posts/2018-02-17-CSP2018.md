---
layout: post
title: ASA Conference on Statistical Practice 2018
category: [R, fitbit]
tags: [R, fitbit]
---

![CSP Conf Logo](/images/csp2018.png "Conference Logo")

Highlights from [Conference on Statistical Practice](https://ww2.amstat.org/meetings/csp/2018/index.cfm) sessions. To keep it simple just a few notes and my favorite slide per session I attended. I work in Population Health Analytics at Legacy Hospital System in Portland, Oregon, so some of these presenters' work may be interpritted in terms of that lense.

## Friday 2/16/2018

### Reflections on Career Opportunities and Leadership in Statistics, *Lisa LaVange, The University of North Carolina*

Leadership - to some extent just do it - statistical training alone provides perspective and a right to contribute. There are "statistical leadership" opportunities and I felt inspired to improve art of of contributing.

Precision medicine is determining the right intervention at right time at the right dose. It struck me care management is precision medecine, or could become it.

### Practical Time-Series Clustering for Messy Data in R *Jonathan Robert Page, University of Hawaii Economic Research Organization*

He took time series data on donations to a projects on a Kenyan GoFundMe type site and looked for clusterings using two methods: Dynamic Time Warping (DTW) using dtwclust and k-Shapes. He uses cumulative sums, not daily donation amounts as the y-value.
After cleaning (for which he presents a weath of R code) it feels easy (maybe he makes it look easy). DTW seems to cluster on when changes occur, so assumes similar changes and measures time distance, x-distance between the key changes, whereas k-shape seems to measure vertical distance at the same time - but not sure these are correct interpritations. 

He likes to present clusters of Time Series in x by x style - so 4, 9, or 16 clusters. He does this to show stakeholders different resolutions / zoom levels. 

![Clusters of donations with k = 9 k-Shapes](/images/smokelines.png "Clusters of donations with k = 9 k-Shapes")

Pretty "smoke graph" I'd call it, with alpha = .01 probably. Blue line is medoid of normalized cluster. Can see in the second one a cluster where no one ever donates anything in the first 30 days. Or Cluster 5, does great at first then flattens out. Clusters 8 and 9 look the most promising - since they finish their first 30 days still with an upward slope, that may indicate continuing interest.

### 	Doing Data Linkage: A Behind-the-Scenes Look *Clinton J. Thompson, National Center for Health Statistics, CDC*



## Saturday 2/17/2018


### Exploring Data Quality and Time Series Event Detection in 2016 US Presidential Election Polls, *[Kaelyn	M.	Rosenburg](https://ww2.amstat.org/meetings/csp/2018/onlineprogram/AbstractDetails.cfm?AbstractID=303685), Reed College*
Problem: Polling implied Clinton would beat Trump. Could survey mode effect explain this error? 

![Polling Clinton v. Trump by phone and by web](/images/clintontrumppolls.png "Polling over time")

This cool plot shows how web vs. phone polling coverged for Clinton over time but Trump's web polling consistently remained higher than phone polling. She suggests this visualizes a possible social desirability bias that could have led to variability in polling estimates.

### Statistical Methods for National Security Risk Quantification and Optimal Resource Allocation, *[Robert Brigantic](https://www.pnnl.gov/science/staff/staff_info.asp?staff_num=8633), Pacific Northwest National Laboratory*

Problem: given multiple risk points and danger situations, how do we best allocate resources to reduce risk? Can we do this intelligently based on data and logic, to supplement expert intuition? 

Pacific Northwest National Lab attempted to solve this problem by quantifying risk. They break it down into quantifiable sub-problems. The final equation, I thought was a nice way to represent it: R = f(C,V,T). This means risk is a function of Consequence, Vulnerability, and Threat. Consequence means how severe is an event - especially in terms of death or injury. Vulnerability means how easy to exploit is a given location? And Threat means likelihood of natural or man-made occurance with potential to harm life, information, or property.

Breaking risk down into these three components allows further breaking it down into measurable sub-components, until it's at a level that analysis can be done. Here they have taken the first component, C, Consequence, broken it down into seven categories, broken a site down into five areas, looked at three possible scenarios and calculated the consequence level of that type of event occuring in that area on that category of consequence on a scale of 1 to 5. The numerical estimate comes with help from data and stakeholders.

![Breaking down risk](/images/pnnl01.png "Risk heat map")

I think this systematic approach could be used to model risk in other questions, such as risk of a hospital admission, or risk of diabetes worsening to other conditions such as end stage renal disease. Instead of five areas, each member of a population would be an "area". Each "security threat" would be a condition that a member could obtain, such as renal disease or cariovascular disease. Consequence could be separated into [the quadruple aim](http://www.annfammed.org/content/12/6/573.full), vulnerability could come from chronic conditions. Threat would be the liklihood of each one, perhaps based on data.


### Causal Inference with Multilevel Data Structures, *[Luke Keele](http://lukekeele.com/), Georgetown University*

Problem: observational studies often are the only realistic option, with bias than prevents normal statistical methods from being reliable. When observational studies have clustering occuring among who receives or doesn't recieve a treatment, that introduces a multilayered issue that makes it yet more complex.

He thinks multilevel matching is more ideal than multilevel regression for clustered observational studies. He has an R package [multiMatch/matchMulti](https://cran.r-project.org/web/packages/matchMulti/index.html) that does this matching work and has several options to handle more complex cases. 

The second study shown was cool. It looked at an actual RCT, but with bias in schools that choose to particpate. He removed the randomly assigned control group, and tried to see if the randomly assigned treatment group could show the same effect in comparison to the non-participating group after adjusting for selection bias. Since the effect size was known, it is a cool example case to see how well the matching algorithm works to get at the treatment effect in an observational study setting. 

Since students could be matched or schools could be matched, it is a multi-level situation requiring more complex matching either by student or school or both. Don't remember the results though.


### A Decision Tool for Causal Inference and Observational Data Analysis Methods in Comparative Effectiveness Research, *[Douglas Landsittel](http://www.dbmi.pitt.edu/node/52371), University of Pittsburgh*

Propensity scores are a prediction problem, but predict on treatment - as opposed to on the response. So they predict if someone received the treatment. In a randomized control trial, an individual's probablity of treatment or control is the same. He suggested propensity scores may be a perfect application of machine learning, since a black box is fine. I've heard this before, and think random forest does make sense, to handle automatic non-linearity managing, and getting to a number quickly. 

He emphasized people use propensity score **methods** not just propensity scores. 
Once having a propensity score, there are four approaches to using them. Of 4 approaches, adjust is the worst. 
One method is inverse of propensity probability as a weight. I thought this looked interesting. There is a lot of work on pros and cons of these methods. He thinks it depends on area of common support and distribution of propensity scores.


### Deploying Quantitative Models as 'Visuals' in Popular Data Visualization Platforms, *[Daniel Fylstra](https://www.solver.com/node/7156), Frontline Systems Inc.*

I use PowerBI at work, and think putting a model in PowerBI may make it less mysterious. I liked his comment of meeting users on their own territory, so to speak. Ideally PowerBI, Tableau, Qlikview, etc. enable "what if" on the spot and rerunning a model in front of stakeholders, leading to a stronger case to implement it.

PowerBI uses TypeScript, a higher level than JavaScript.

His advice about "big data" is it could be a bubble - that if you want to learn Spark or Python or R to use it right now, go for it, but that it seems in two years they may be replaced by something else (I'm thinking Julia and SQL wrappers are contenters). So what matters in his view is the fundamentals. I interpritted this as the fundamentals of problem solving with data. 


